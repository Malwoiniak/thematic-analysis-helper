{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules, libraries\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.models import Word2Vec, LdaMulticore\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import glob\n",
    "import pyLDAvis\n",
    "from pyLDAvis import gensim \n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data into DataFrame: concatenate scraped data from all years for jackbones.com\n",
    "df = pd.concat([pd.read_csv(f) for f in glob.glob('libdem scrape/*.csv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of all sentences in summary content\n",
    "content = ' '.join(df['summary'])\n",
    "sentences = TextBlob(content).sentences\n",
    "sent_list=[]\n",
    "for s in sentences:\n",
    "    sent_list.append(str(s))\n",
    "    \n",
    "#write all sentences to .txt  \n",
    "with open('libdem.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in sent_list:\n",
    "        f.write(line)\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentence):\n",
    "    \"\"\" Converts a document into a list of lowercase tokens, ignoring tokens that are too short or too long and stopwords.\"\"\"\n",
    "    return [word for word in simple_preprocess(sentence) \\\n",
    "            if word not in STOPWORDS]\n",
    "\n",
    "def read_sentences(filename):\n",
    "    \"\"\" Performs def preprocessing on given file one sentence (line) and the time\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        for line in f:\n",
    "            yield preprocessing(line)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of preprocessed sentences for libdem.com (all years)\n",
    "%time sentences = list(read_sentences('libdem.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train word2vec model (window, min_count: adjust if needed)\n",
    "%time model = Word2Vec(sentences, window=1, min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get synonyms in trained model\n",
    "model.wv.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform mathematical operation on words: amazing+good-bad\n",
    "model.wv.most_similar(positive=['amazing', 'good'], negative=['bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get randomly permuted sentences \n",
    "sentences_light = np.random.permutation(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get 3000 of randomly permuted sentences\n",
    "sentences_light = sentences_light[:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get dictionary of normalized words and their ids \n",
    "%time dictionary = Dictionary(sentences_light)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert doc into bag-of-words (BoW) for each sentence in sentences_light\n",
    "%time bow_corpus = [dictionary.doc2bow(sent) for sent in sentences_light]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train lda_model\n",
    "%time lda_model = LdaMulticore(bow_corpus, id2word=dictionary, num_topics=100, passes=20, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show topics, index in trained lda_model\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {}\\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize topics with pyLDAvis\n",
    "lda_vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)\n",
    "pyLDAvis.display(lda_vis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
